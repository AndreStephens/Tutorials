{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Automating Speech-to-Text Transcription\n",
    "\n",
    "UX researchers and enthnogoraphers frequently conduct interviews in order to gather data. However, audio recordings are extremely tedious and costly to transcribe. Many might be surprised to learn that the first completely-automatic translation was done all the way back in 1954. Although it was limited (with a mere six grammar rules and 250 lexical items in its vocabulary), the experiment developed jointly by Georgetown University and IBM translated more than sixty Russian sentences into English. Interestingly enought, Leon Dostert, one of the scientists involved in the experiment, predicted that generalized machine translation would be realized in \"five, perhaps, three years\" <sup>[1](#myfootnote1)</sup>. Alas, progress was more slower than the sanguine scientist suggested.\n",
    "\n",
    "Advancements in machine learning and the exponential growth in computating power, however, have finally made machine translation a reality, albeit a perfectible one. Powered by the same speech recognition technology that works on your android and Chrome OS devices, the Google Cloud Platform provides the best speech-to-text conversion tool available today. And it's practically free! \n",
    "\n",
    "Audio recordings under 60 minutes are free and recordings over 60 minutes are USD \\$0.024/minute. A three-hour interview would therefore cost you only $4.32. And what's to stop you from [splitting the audio file](https://www.cnet.com/news/how-to-split-audio-tracks/) into 3 or 4 chunks?\n",
    "\n",
    "I am going to show you how -- with little to no computing experience -- you can use it right now!\n",
    "\n",
    "Let's get to it...\n",
    "\n",
    "## Pre-processing\n",
    "\n",
    "**Note**: The Speech-to-Text API has [certain requirements](https://cloud.google.com/speech-to-text/docs/reference/rest/v1/RecognitionConfig) in order to work. Notably, the audio file must be encoded in a suitable format (e.g. .flac, .wav) and supports only 1 channel (i.e. mono, not stereo). The API supports sample rates from 8000 to 48000Hz. For best results, at least 16000Hz is recommended. I will shortly provide a script for encoding files as FLAC.\n",
    "\n",
    "You will also need to set up a Google Cloud Platform account which will then allow you to create a bucket to store your audio file on the cloud and request an api key. The `gcs_uri` object is the link to your file in the bucket and `cred_key` is the api key stored as a ."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How it works\n",
    "\n",
    "#### Get modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Get modules from Google Cloud client package"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from google.cloud import speech\n",
    "from google.cloud.speech import enums\n",
    "from google.cloud.speech import types"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Iniitiate the client\n",
    "\n",
    "I highly recommend storing your API key as a JSON file if you plan to run this from command line."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "cred_key = 'C:\\myPath\\myAPI-key.json' # api key json file\n",
    "os.environ[\"GOOGLE_APPLICATION_CREDENTIALS\"] = cred_key\n",
    "client = speech.SpeechClient()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load link to audio file from Google Cloud Platform\n",
    "\n",
    "Here you can specify the language and dialect you are using. The complete list of supported regional languages can be found on Google Cloud's [language support page](https://cloud.google.com/speech-to-text/docs/languages)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "gcs_uri = 'gs://mybucket/myaudio.flac' # must include full path if audio is inside bucket directories\n",
    "lang_code = 'en-US' # Use 'en-US' for US English\n",
    "\n",
    "audio = types.RecognitionAudio(uri=gcs_uri)\n",
    "config = types.RecognitionConfig(\n",
    "    encoding=enums.RecognitionConfig.AudioEncoding.FLAC, \n",
    "    sample_rate_hertz=48000,\n",
    "    language_code=lang_code \n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Detect Speech"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "operation = client.long_running_recognize(config, audio)\n",
    "response = operation.result(timeout=None) # results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Obtain Results\n",
    "Process results to return a dictionary with the transcribed text and degree of confidence for each operation and list of transcription results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "transcript_list = []\n",
    "result_dict = {}\n",
    "\n",
    "for n, result in enumerate(response.results):\n",
    "    transcript = result.alternatives[0].transcript\n",
    "    confidence = result.alternatives[0].confidence\n",
    "\n",
    "    result_value = {\n",
    "        'transcript': transcript,\n",
    "        'confidence': confidence\n",
    "    }\n",
    "\n",
    "    # dictionary with transcription and confidence interval\n",
    "    result_key = 'result_{}'.format(n)\n",
    "    result_dict[result_key] = result_value\n",
    "\n",
    "    # joined list of transcribed data\n",
    "    transcript_list.append(transcript)\n",
    "    joined_transcript_list = ' |'.join(transcript_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Write file to system"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "json_filename = 'audio_transcription_results.json'\n",
    "text_filename = 'audio_transcription_results.txt'\n",
    "\n",
    "with open(json_filename, 'w') as f:\n",
    "    f.write(json.dumps(result_dict))\n",
    "    \n",
    "with open(text_filename, 'w') as f:\n",
    "        f.write(str(joined_transcript_list)) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### References\n",
    "\n",
    "<a name=\"myfootnote1\">1</a>: Hutchins (2004). [\"The Georgetown-IBM Experiment Demonstrated in January 1954.\"](http://www.hutchinsweb.me.uk/AMTA-2004.pdf) in *Machine Translation: From Real Users to Research*, pp 102-114."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
